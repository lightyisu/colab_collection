{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 30716,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "notebook0ef402ff6d",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lightyisu/colab_collection/blob/main/dna%E5%88%86%E5%89%B2%E5%AE%9E%E9%AA%8C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from transformers import AutoModel\n",
        "\n",
        "from memory_profiler import profile\n",
        "class BERTModel(nn.Module):\n",
        "    def __init__(self, bert_path, label_count):\n",
        "        super(BERTModel, self).__init__()\n",
        "        self.bert = AutoModel.from_pretrained('/kaggle/working/DNABERT-2-117M',trust_remote_code=True)\n",
        "        self.num_labels = label_count\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.loss_func = nn.CrossEntropyLoss()\n",
        "        self.linear = nn.Linear(768, label_count)\n",
        "    def forward(self, input_ids=None, label_ids=None, mask=None):\n",
        "        print('oxp')\n",
        "        print(input_ids.shape)\n",
        "        input_temp=input_ids\n",
        "        outputs = self.bert(input_ids=input_ids)\n",
        "        print('op',outputs[0])\n",
        "        # sequence_output = outputs[0]\n",
        "        # sequence_output = self.dropout(sequence_output)\n",
        "        # logits = self.linear(sequence_output)\n",
        "        # outputs = (logits,) + outputs[2:]\n",
        "        # active_loss = mask.view(-1) == 1\n",
        "\n",
        "        # if label_ids is not None:\n",
        "        #     if mask is not None:\n",
        "        #         active_loss = mask.view(-1) == 1\n",
        "        #         active_logits = logits.view(-1, self.num_labels)[active_loss]\n",
        "        #         active_labels = label_ids.view(-1)[active_loss]\n",
        "        #         loss = self.loss_func(active_logits, active_labels)\n",
        "        #     outputs = (loss,) + outputs\n",
        "\n",
        "        return outputs\n",
        "\n",
        "class BERTTest(nn.Module):\n",
        "    def __init__(self, bert_path, label_count):\n",
        "        super(BERTTest, self).__init__()\n",
        "        self.num_labels = label_count\n",
        "        self.bert =AutoModel.from_pretrained(bert_path,trust_remote_code=True)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.linear = nn.Linear(768, label_count)\n",
        "    def forward(self, input_ids=None, mask=None):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=mask)\n",
        "        sequence_output = outputs[0]\n",
        "        sequence_output = self.dropout(sequence_output)\n",
        "        logits = self.linear(sequence_output)\n",
        "        if mask is not None:\n",
        "            active_mask = mask.view(-1) == 1\n",
        "            active_logits = logits.view(-1, self.num_labels)[active_mask]\n",
        "        return  active_logits"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2024-06-04T12:25:30.804329Z",
          "iopub.execute_input": "2024-06-04T12:25:30.804703Z",
          "iopub.status.idle": "2024-06-04T12:25:30.816473Z",
          "shell.execute_reply.started": "2024-06-04T12:25:30.804675Z",
          "shell.execute_reply": "2024-06-04T12:25:30.8154Z"
        },
        "trusted": true,
        "id": "tITiwWCZkbgX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_temp=None"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-04T12:25:04.300499Z",
          "iopub.execute_input": "2024-06-04T12:25:04.301345Z",
          "iopub.status.idle": "2024-06-04T12:25:04.305197Z",
          "shell.execute_reply.started": "2024-06-04T12:25:04.301312Z",
          "shell.execute_reply": "2024-06-04T12:25:04.304165Z"
        },
        "trusted": true,
        "id": "FTYgJduKkbgZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import torch\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "label2id = {\n",
        "    \"0\": 0,\n",
        "    \"1\": 1\n",
        "\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class DataProcess():\n",
        "    def __init__(self, data_path, data_type):\n",
        "        self.data_dir = os.path.join(data_path, data_type+'.npz')\n",
        "    def process(self):\n",
        "        data = np.load(self.data_dir, allow_pickle=True)\n",
        "        data_df = pd.concat([pd.DataFrame(data['words'], columns=['words']),\n",
        "                            pd.DataFrame(data['labels'], columns=['labels'])],axis=1)\n",
        "        data_df = data_df.dropna()\n",
        "        # data_df['labels'] = data_df['labels'].map(lambda x: self.trans(x))\n",
        "        corpus = []\n",
        "        for _, row in data_df.iterrows():\n",
        "            words = row['words']\n",
        "            labels = row['labels']\n",
        "            corpus.append((words, labels))\n",
        "        return corpus\n",
        "\n",
        "    # def trans(self, labels):\n",
        "    #     labels = list(labels)\n",
        "    #     nums = [label2id[label] for label in labels]\n",
        "    #     return nums\n",
        "\n",
        "\n",
        "\n",
        "class CluenerDataset(Dataset):\n",
        "    def __init__(self, corpus, tokenizer=None, seq_len=50):\n",
        "        super(CluenerDataset, self).__init__()\n",
        "        self.corpus = corpus\n",
        "        self.tokenizer = tokenizer\n",
        "        self.seq_len = seq_len\n",
        "        self.len = len(corpus)\n",
        "\n",
        "\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        sentence, label_ids = self.corpus[item]\n",
        "\n",
        "        # tokens = ['[CLS]'] + tokens + ['[SEP]']\n",
        "        # label_ids = [0] + label_ids + [0]\n",
        "\n",
        "        max_length = 312\n",
        "        input_ids = self.tokenizer(sentence, return_tensors='pt',padding='max_length', truncation=True,max_length=max_length)[\"input_ids\"]\n",
        "#         print('batch',input_ids)\n",
        "\n",
        "        label_ids=[int(label) for label in label_ids]\n",
        "\n",
        "        return {\"input_ids\": input_ids ,\n",
        "                        \"label_ids\": torch.IntTensor(label_ids) }\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "\n",
        "\n",
        "\n",
        "def build_loader(data_path, data_type,tokenizer=None,seq_len=50):\n",
        "    dataprocess = DataProcess(data_path, data_type)\n",
        "    corpus = dataprocess.process()\n",
        "    dataset = CluenerDataset(corpus,tokenizer,seq_len)\n",
        "    data_loader = DataLoader(dataset, batch_size=8,shuffle=False)\n",
        "    return data_loader"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-04T12:22:26.845331Z",
          "iopub.execute_input": "2024-06-04T12:22:26.845709Z",
          "iopub.status.idle": "2024-06-04T12:22:26.859648Z",
          "shell.execute_reply.started": "2024-06-04T12:22:26.845681Z",
          "shell.execute_reply": "2024-06-04T12:22:26.858428Z"
        },
        "trusted": true,
        "id": "q3zJjc6ekbga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "from tqdm import trange\n",
        "import os\n",
        "# os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "import torch, gc\n",
        "\n",
        "\n",
        "\n",
        "def train_model(model, epochs, train_loader,save_path,log_dir, device):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
        "    for epoch in trange(epochs):\n",
        "        tr_loss, n_steps, correct_preds, total_preds = 0, 0, 0, 0\n",
        "        model.train()\n",
        "        writer = SummaryWriter(log_dir=log_dir + f'/run_{epoch}')\n",
        "        progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), leave=False)\n",
        "        for _, batch in progress_bar:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            label_ids = batch['label_ids'].to(device)\n",
        "            # mask = batch['attn_mask'].to(device)\n",
        "            print('start-----')\n",
        "            output = model(input_ids, label_ids )\n",
        "            print('end-----')\n",
        "            break;\n",
        "        epoch_loss = tr_loss / n_steps\n",
        "        epoch_acc = correct_preds / total_preds\n",
        "        print(f'Epoch {epoch + 1}/{epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}')\n",
        "        writer.add_scalar('Train/Loss', epoch_loss, epoch + 1)\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            torch.save(model.state_dict(), f'{save_path}/{epoch + 1}.pt')\n",
        "    writer.close()\n",
        "\n",
        "def main():\n",
        "    bert_path = \"/kaggle/working/DNABERT-2-117M\"\n",
        "    save_path = '/kaggle/working/dnaseq'\n",
        "    log_dir = '/kaggle/working/dnaseq'\n",
        "    input_path = \"/kaggle/working\"\n",
        "    data_type = \"train\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(bert_path,trust_remote_code=True)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    train_loader = build_loader(data_path= input_path,data_type=data_type,tokenizer=tokenizer, seq_len=310)\n",
        "    model = BERTModel(bert_path=bert_path,label_count=2).to(device)\n",
        "    train_model(model, 100, train_loader, save_path, log_dir, device)\n",
        "\n",
        "\n",
        "main()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-04T12:22:49.982694Z",
          "iopub.execute_input": "2024-06-04T12:22:49.98309Z",
          "iopub.status.idle": "2024-06-04T12:22:57.096535Z",
          "shell.execute_reply.started": "2024-06-04T12:22:49.983061Z",
          "shell.execute_reply": "2024-06-04T12:22:57.095098Z"
        },
        "trusted": true,
        "id": "SQdEa0Izkbga",
        "outputId": "0b4cf9b0-2e15-4521-c3b5-553da9d728ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "/root/.cache/huggingface/modules/transformers_modules/DNABERT-2-117M/bert_layers.py:126: UserWarning: Unable to import Triton; defaulting MosaicBERT attention implementation to pytorch (this will reduce throughput when using this model).\n  warnings.warn(\nSome weights of BertModel were not initialized from the model checkpoint at /kaggle/working/DNABERT-2-117M and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n  0%|          | 0/100 [00:00<?, ?it/s]\n  0%|          | 0/6660 [00:00<?, ?it/s]\u001b[A\n  0%|          | 0/100 [00:00<?, ?it/s] \u001b[A",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "start-----\noxp\ntorch.Size([8, 300, 312])\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[35], line 53\u001b[0m\n\u001b[1;32m     49\u001b[0m     model \u001b[38;5;241m=\u001b[39m BERTModel(bert_path\u001b[38;5;241m=\u001b[39mbert_path,label_count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     50\u001b[0m     train_model(model, \u001b[38;5;241m100\u001b[39m, train_loader, save_path, log_dir, device)\n\u001b[0;32m---> 53\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[35], line 50\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m build_loader(data_path\u001b[38;5;241m=\u001b[39m input_path,data_type\u001b[38;5;241m=\u001b[39mdata_type,tokenizer\u001b[38;5;241m=\u001b[39mtokenizer, seq_len\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m310\u001b[39m)\n\u001b[1;32m     49\u001b[0m model \u001b[38;5;241m=\u001b[39m BERTModel(bert_path\u001b[38;5;241m=\u001b[39mbert_path,label_count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 50\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[35], line 29\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, epochs, train_loader, save_path, log_dir, device)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# mask = batch['attn_mask'].to(device)\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstart-----\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 29\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_ids\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mend-----\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m;\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[34], line 16\u001b[0m, in \u001b[0;36mBERTModel.forward\u001b[0;34m(self, input_ids, label_ids, mask)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moxp\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(input_ids\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 16\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mop\u001b[39m\u001b[38;5;124m'\u001b[39m,outputs[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# sequence_output = outputs[0]\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# sequence_output = self.dropout(sequence_output)\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# logits = self.linear(sequence_output)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m#         loss = self.loss_func(active_logits, active_labels)\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m#     outputs = (loss,) + outputs\u001b[39;00m\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/DNABERT-2-117M/bert_layers.py:596\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, token_type_ids, attention_mask, position_ids, output_all_encoded_layers, masked_tokens_mask, **kwargs)\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m token_type_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    594\u001b[0m     token_type_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(input_ids)\n\u001b[0;32m--> 596\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    599\u001b[0m subset_mask \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    600\u001b[0m first_col_mask \u001b[38;5;241m=\u001b[39m []\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/DNABERT-2-117M/bert_layers.py:99\u001b[0m, in \u001b[0;36mBertEmbeddings.forward\u001b[0;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[1;32m     96\u001b[0m     inputs_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mword_embeddings(input_ids)\n\u001b[1;32m     97\u001b[0m token_type_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_type_embeddings(token_type_ids)\n\u001b[0;32m---> 99\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[43minputs_embeds\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtoken_type_embeddings\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m# no position embeddings! ALiBi\u001b[39;00m\n\u001b[1;32m    101\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(embeddings)\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.14 GiB. GPU 0 has a total capacty of 14.75 GiB of which 1.31 GiB is free. Process 3256 has 13.44 GiB memory in use. Of the allocated memory 11.62 GiB is allocated by PyTorch, and 1.71 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ],
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 2.14 GiB. GPU 0 has a total capacty of 14.75 GiB of which 1.31 GiB is free. Process 3256 has 13.44 GiB memory in use. Of the allocated memory 11.62 GiB is allocated by PyTorch, and 1.71 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
          "output_type": "error"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "14zd6kJSkbgb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://pan.tenire.com/down.php/e6f91824c1a792446bd9d8afb45bcdee.npz -O train.npz"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-04T12:03:06.005459Z",
          "iopub.execute_input": "2024-06-04T12:03:06.00593Z",
          "iopub.status.idle": "2024-06-04T12:03:08.93041Z",
          "shell.execute_reply.started": "2024-06-04T12:03:06.005887Z",
          "shell.execute_reply": "2024-06-04T12:03:08.929017Z"
        },
        "trusted": true,
        "id": "TfmBSI2Kkbgb",
        "outputId": "82d21044-b888-47b3-e57c-9a25702d3eb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "--2024-06-04 12:03:07--  https://pan.tenire.com/down.php/e6f91824c1a792446bd9d8afb45bcdee.npz\nResolving pan.tenire.com (pan.tenire.com)... 188.114.97.0, 188.114.96.0, 2a06:98c1:3121::, ...\nConnecting to pan.tenire.com (pan.tenire.com)|188.114.97.0|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 64784459 (62M) [application/force-download]\nSaving to: 'train.npz'\n\ntrain.npz           100%[===================>]  61.78M  39.4MB/s    in 1.6s    \n\n2024-06-04 12:03:08 (39.4 MB/s) - 'train.npz' saved [64784459/64784459]\n\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-04T12:13:12.702331Z",
          "iopub.execute_input": "2024-06-04T12:13:12.702715Z",
          "iopub.status.idle": "2024-06-04T12:13:13.390062Z",
          "shell.execute_reply.started": "2024-06-04T12:13:12.702687Z",
          "shell.execute_reply": "2024-06-04T12:13:13.388951Z"
        },
        "trusted": true,
        "id": "WLZU9YC0kbgb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git lfs clone https://huggingface.co/zhihan1996/DNABERT-2-117M"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-04T12:06:27.305866Z",
          "iopub.execute_input": "2024-06-04T12:06:27.306497Z",
          "iopub.status.idle": "2024-06-04T12:06:40.820253Z",
          "shell.execute_reply.started": "2024-06-04T12:06:27.306465Z",
          "shell.execute_reply": "2024-06-04T12:06:40.819034Z"
        },
        "trusted": true,
        "id": "2Z012gZvkbgc",
        "outputId": "dee61995-8399-40ad-bb19-cbe562c6e9d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "WARNING: 'git lfs clone' is deprecated and will not be updated\n          with new flags from 'git clone'\n\n'git clone' has been updated in upstream Git to have comparable\nspeeds to 'git lfs clone'.\nCloning into 'DNABERT-2-117M'...\nremote: Enumerating objects: 68, done.\u001b[K\nremote: Counting objects: 100% (46/46), done.\u001b[K\nremote: Compressing objects: 100% (41/41), done.\u001b[K\nremote: Total 68 (delta 28), reused 5 (delta 5), pack-reused 22 (from 1)\u001b[K\nUnpacking objects: 100% (68/68), 83.60 KiB | 2.79 MiB/s, done.\nDownloading LFS objects: 100% (1/1), 468 MB | 53 MB/s                           \r",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install einops"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-04T12:07:31.863259Z",
          "iopub.execute_input": "2024-06-04T12:07:31.863935Z",
          "iopub.status.idle": "2024-06-04T12:07:45.576709Z",
          "shell.execute_reply.started": "2024-06-04T12:07:31.863904Z",
          "shell.execute_reply": "2024-06-04T12:07:45.575603Z"
        },
        "trusted": true,
        "id": "3kp2BmRdkbgc",
        "outputId": "4d384c07-4a30-4a7c-c451-0d3004e1930e"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Collecting einops\n  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\nDownloading einops-0.8.0-py3-none-any.whl (43 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m629.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h\u001b[33mWARNING: Error parsing requirements for aiohttp: [Errno 2] No such file or directory: '/opt/conda/lib/python3.10/site-packages/aiohttp-3.9.1.dist-info/METADATA'\u001b[0m\u001b[33m\n\u001b[0mInstalling collected packages: einops\nSuccessfully installed einops-0.8.0\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git lfs clone https://huggingface.co/zhihan1996/DNABERT-2-117M"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-06T07:03:58.136103Z",
          "iopub.execute_input": "2024-06-06T07:03:58.136508Z",
          "iopub.status.idle": "2024-06-06T07:03:59.409953Z",
          "shell.execute_reply.started": "2024-06-06T07:03:58.136477Z",
          "shell.execute_reply": "2024-06-06T07:03:59.408101Z"
        },
        "trusted": true,
        "id": "gTR0qjazkbgc",
        "outputId": "a21bdbdd-8a72-4dc5-b357-21db15f522ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "WARNING: 'git lfs clone' is deprecated and will not be updated\n          with new flags from 'git clone'\n\n'git clone' has been updated in upstream Git to have comparable\nspeeds to 'git lfs clone'.\nfatal: destination path 'DNABERT-2-117M' already exists and is not an empty directory.\nError(s) during clone:\ngit clone failed: exit status 128\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install einops"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-06T07:12:14.784246Z",
          "iopub.execute_input": "2024-06-06T07:12:14.784673Z",
          "iopub.status.idle": "2024-06-06T07:12:30.891333Z",
          "shell.execute_reply.started": "2024-06-06T07:12:14.784641Z",
          "shell.execute_reply": "2024-06-06T07:12:30.889082Z"
        },
        "trusted": true,
        "id": "o1wCDyxIkbgc",
        "outputId": "858dc1ac-9774-4e64-ffa1-ec1302aee65d"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Collecting einops\n  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\nDownloading einops-0.8.0-py3-none-any.whl (43 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[33mWARNING: Error parsing requirements for aiohttp: [Errno 2] No such file or directory: '/opt/conda/lib/python3.10/site-packages/aiohttp-3.9.1.dist-info/METADATA'\u001b[0m\u001b[33m\n\u001b[0mInstalling collected packages: einops\nSuccessfully installed einops-0.8.0\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"/kaggle/working/DNABERT-2-117M\",trust_remote_code=True)\n",
        "model = AutoModel.from_pretrained(\"/kaggle/working/DNABERT-2-117M\",trust_remote_code=True)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-06T07:55:01.965916Z",
          "iopub.execute_input": "2024-06-06T07:55:01.966332Z",
          "iopub.status.idle": "2024-06-06T07:55:02.596614Z",
          "shell.execute_reply.started": "2024-06-06T07:55:01.966303Z",
          "shell.execute_reply": "2024-06-06T07:55:02.595383Z"
        },
        "trusted": true,
        "id": "zFI_S2oCkbgd",
        "outputId": "8059816e-0008-4412-8482-7395b21595b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "Some weights of BertModel were not initialized from the model checkpoint at /kaggle/working/DNABERT-2-117M and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "def _tokenize_extend_labels(sentence):\n",
        "        tokens = []\n",
        "        for word in sentence:\n",
        "            tokenized_word = tokenizer.tokenize(word)\n",
        "            tokens.extend(tokenized_word)\n",
        "        return tokens\n",
        "# str=['生', '生', '不', '息', 'C', 'S', 'O', 'L', '生', '化', '狂', '潮', '让', '你', '填', '弹', '狂', '扫']\n",
        "str=['A', 'C', 'G', 'T', 'A', 'G', 'C', 'A', 'T', 'C', 'G', 'A', 'C', 'A', 'C', 'T', 'T', 'G', 'G', 'T', 'T', 'A', 'T', 'C', 'G', 'A', 'T', 'A', 'G', 'C']\n",
        "# tokens=tokenizer.tokenize(str)\n",
        "\n",
        "tokens=_tokenize_extend_labels(str)\n",
        "tokens = ['[CLS]'] + tokens + ['[SEP]']\n",
        "print(tokens)\n",
        "#directly tokenizer -shape([18,3]) 3-single tokenzer-id\n",
        "#([[ 101, 4495,  102],\n",
        "#[ 101, 4495,  102]])\n",
        "inputs_id=tokenizer.convert_tokens_to_ids(tokens)\n",
        "inputs_id=torch.tensor(inputs_id, dtype=torch.long)\n",
        "inputs_id=inputs_id.unsqueeze(1)\n",
        "# inputs_id=tokenizer(str)[\"input_ids\"]\n",
        "# inputs_id=tokenizer(str)[\"input_ids\"]\n",
        "hidden_states = model(inputs_id)[0] # [1, sequence_length, 768]\n",
        "# print(inputs_id)\n",
        "print('hidden_state',hidden_states)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-06T07:55:32.730425Z",
          "iopub.execute_input": "2024-06-06T07:55:32.731167Z",
          "iopub.status.idle": "2024-06-06T07:55:32.946401Z",
          "shell.execute_reply.started": "2024-06-06T07:55:32.731073Z",
          "shell.execute_reply": "2024-06-06T07:55:32.944191Z"
        },
        "trusted": true,
        "id": "ihtUhSDxkbgd",
        "outputId": "d39bd3c2-c3c0-43cc-f57f-843559cea8b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "['[CLS]', 'A', 'C', 'G', 'T', 'A', 'G', 'C', 'A', 'T', 'C', 'G', 'A', 'C', 'A', 'C', 'T', 'T', 'G', 'G', 'T', 'T', 'A', 'T', 'C', 'G', 'A', 'T', 'A', 'G', 'C', '[SEP]']\nhidden_state tensor([[[ 0.0251, -0.1064, -0.0880,  ..., -0.0253,  0.0708,  0.0632]],\n\n        [[ 0.0490,  0.0624,  0.2037,  ..., -0.0567,  0.1172, -0.0316]],\n\n        [[ 0.1169,  0.0800,  0.5125,  ...,  0.1701,  0.3980,  0.2334]],\n\n        ...,\n\n        [[-0.0821, -0.1935,  0.0383,  ...,  0.0619,  0.1893,  0.0249]],\n\n        [[ 0.1169,  0.0800,  0.5125,  ...,  0.1701,  0.3980,  0.2334]],\n\n        [[-0.0585, -0.1229, -0.1667,  ..., -0.0962,  0.0483, -0.0757]]],\n       grad_fn=<ViewBackward0>)\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "str=\"ACGTAGCATCGGATCTATCTATCGACACTTGGTTATCGATCTACGAGCATCTCGTTAGC\"\n",
        "print([item for item in str])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-06T07:05:16.272702Z",
          "iopub.execute_input": "2024-06-06T07:05:16.273276Z",
          "iopub.status.idle": "2024-06-06T07:05:16.283356Z",
          "shell.execute_reply.started": "2024-06-06T07:05:16.273235Z",
          "shell.execute_reply": "2024-06-06T07:05:16.281308Z"
        },
        "trusted": true,
        "id": "khQsx7tkkbgd",
        "outputId": "fe6ca839-221c-4503-a236-eb4af1a8235c"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "['A', 'C', 'G', 'T', 'A', 'G', 'C', 'A', 'T', 'C', 'G', 'G', 'A', 'T', 'C', 'T', 'A', 'T', 'C', 'T', 'A', 'T', 'C', 'G', 'A', 'C', 'A', 'C', 'T', 'T', 'G', 'G', 'T', 'T', 'A', 'T', 'C', 'G', 'A', 'T', 'C', 'T', 'A', 'C', 'G', 'A', 'G', 'C', 'A', 'T', 'C', 'T', 'C', 'G', 'T', 'T', 'A', 'G', 'C']\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-chinese\",trust_remote_code=True)\n",
        "model = AutoModel.from_pretrained(\"google-bert/bert-base-chinese\",trust_remote_code=True)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-06T07:31:16.890855Z",
          "iopub.execute_input": "2024-06-06T07:31:16.891376Z",
          "iopub.status.idle": "2024-06-06T07:31:20.104791Z",
          "shell.execute_reply.started": "2024-06-06T07:31:16.891339Z",
          "shell.execute_reply": "2024-06-06T07:31:20.103231Z"
        },
        "trusted": true,
        "id": "KVo9REobkbge",
        "outputId": "98691e6b-ca6b-4196-87dd-a8fbbd7c7685",
        "colab": {
          "referenced_widgets": [
            "3215f07f2d3745a69234121641027cdf",
            "46b9c4ead9094485bef98efa4abe557f",
            "76b39589e9c041b2a1840c197cca8346",
            "561e3eb5a64f4f978c4fcf00ed684cad",
            "a5bbf34c4bfc426d9386bec6ca7c9a41"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3215f07f2d3745a69234121641027cdf"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json:   0%|          | 0.00/624 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "46b9c4ead9094485bef98efa4abe557f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "vocab.txt:   0%|          | 0.00/110k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "76b39589e9c041b2a1840c197cca8346"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.json:   0%|          | 0.00/269k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "561e3eb5a64f4f978c4fcf00ed684cad"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model.safetensors:   0%|          | 0.00/412M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a5bbf34c4bfc426d9386bec6ca7c9a41"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "def _tokenize_extend_labels(sentence):\n",
        "        tokens = []\n",
        "        for word in sentence:\n",
        "            tokenized_word = tokenizer.tokenize(word)\n",
        "            tokens.extend(tokenized_word)\n",
        "        return tokens\n",
        "str=['生', '生', '不', '息', 'C', 'S', 'O', 'L', '生', '化', '狂', '潮', '让', '你', '填', '弹', '狂', '扫']\n",
        "# str=['A', 'C', 'G', 'T', 'A', 'G', 'C', 'A', 'T', 'C', 'G', 'A', 'C', 'A', 'C', 'T', 'T', 'G', 'G', 'T', 'T', 'A', 'T', 'C', 'G', 'A', 'T', 'A', 'G', 'C']\n",
        "# tokens=tokenizer.tokenize(str)\n",
        "\n",
        "tokens=_tokenize_extend_labels(str)\n",
        "tokens = ['[CLS]'] + tokens + ['[SEP]']\n",
        "print(tokens)\n",
        "#directly tokenizer -shape([18,3]) 3-single tokenzer-id\n",
        "#([[ 101, 4495,  102],\n",
        "#[ 101, 4495,  102]])\n",
        "inputs_id=tokenizer.convert_tokens_to_ids(tokens)\n",
        "inputs_id=torch.tensor(inputs_id, dtype=torch.long)\n",
        "inputs_id=inputs_id.unsqueeze(1)\n",
        "# inputs_id=tokenizer(str)[\"input_ids\"]\n",
        "# inputs_id=tokenizer(str)[\"input_ids\"]\n",
        "hidden_states = model(inputs_id)[0] # [1, sequence_length, 768]\n",
        "print(inputs_id)\n",
        "print('hidden_state',hidden_states)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-06T07:43:30.342398Z",
          "iopub.execute_input": "2024-06-06T07:43:30.342861Z",
          "iopub.status.idle": "2024-06-06T07:43:30.478017Z",
          "shell.execute_reply.started": "2024-06-06T07:43:30.342825Z",
          "shell.execute_reply": "2024-06-06T07:43:30.476532Z"
        },
        "trusted": true,
        "id": "ZmktP1UMkbge",
        "outputId": "f76f39b7-deb1-4150-95e6-0a84ee4370e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "['[CLS]', '生', '生', '不', '息', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '生', '化', '狂', '潮', '让', '你', '填', '弹', '狂', '扫', '[SEP]']\ntensor([[ 101],\n        [4495],\n        [4495],\n        [ 679],\n        [2622],\n        [ 100],\n        [ 100],\n        [ 100],\n        [ 100],\n        [4495],\n        [1265],\n        [4312],\n        [4060],\n        [6375],\n        [ 872],\n        [1856],\n        [2486],\n        [4312],\n        [2812],\n        [ 102]])\nhidden_state tensor([[[ 0.1157,  0.3340, -0.3357,  ...,  1.7525, -0.5618,  0.3029]],\n\n        [[ 1.0068,  0.6529,  0.5228,  ...,  0.9169, -0.1935, -0.1904]],\n\n        [[ 1.0068,  0.6529,  0.5228,  ...,  0.9169, -0.1935, -0.1904]],\n\n        ...,\n\n        [[ 0.6826,  1.0743, -0.4017,  ...,  1.3547, -0.3943,  0.0812]],\n\n        [[ 0.1855, -0.4305,  0.0671,  ...,  0.9354,  0.4295, -0.1573]],\n\n        [[ 0.1160,  0.3337, -0.3363,  ...,  1.7524, -0.5612,  0.3031]]],\n       grad_fn=<NativeLayerNormBackward0>)\n",
          "output_type": "stream"
        }
      ]
    }
  ]
}